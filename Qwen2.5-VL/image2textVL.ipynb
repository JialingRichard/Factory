{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2755f1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chen/dev/Factory/Qwen2.5-VL\n",
      "cookbooks\t  image2textVL.ipynb  README.md\n",
      "docker\t\t  LICENSE\t      requirements_web_demo.txt\n",
      "downloadmodel.py  qwen-vl-finetune    web_demo_mm.py\n",
      "evaluation\t  qwen-vl-utils       web_demo_streaming\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0737b9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "# from qwen_vl_utils import process_vision_info\n",
    "# import torch\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# # default: Load the model on the available device(s)\n",
    "# # model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "# #     \"Qwen/Qwen2.5-VL-3B-Instruct-AWQ\", \n",
    "# #     torch_dtype= torch.float16, \n",
    "# #     device_map=\"auto\",\n",
    "# #     # attn_implementation=\"sdpa\",\n",
    "# # )\n",
    "# # print dtype and device map\n",
    "# # print(f\"Model dtype: {model.dtype}\")\n",
    "# # print(f\"Model device map: {model.hf_device_map}\")\n",
    "\n",
    "# # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-VL-3B-Instruct-AWQ\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# # default processer\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct-AWQ\")\n",
    "\n",
    "# # The default range for the number of visual tokens per image in the model is 4-16384.\n",
    "# # You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
    "# # min_pixels = 256*28*28\n",
    "# # max_pixels = 1280*28*28\n",
    "# # processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct-AWQ\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\n",
    "#                 \"type\": \"image\",\n",
    "#                 \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "#             },\n",
    "#             {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "#         ],\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# # Preparation for inference\n",
    "# text = processor.apply_chat_template(\n",
    "#     messages, tokenize=False, add_generation_prompt=True\n",
    "# )\n",
    "# image_inputs, video_inputs = process_vision_info(messages)\n",
    "# inputs = processor(\n",
    "#     text=[text],\n",
    "#     images=image_inputs,\n",
    "#     videos=video_inputs,\n",
    "#     padding=True,\n",
    "#     return_tensors=\"pt\",\n",
    "# )\n",
    "# inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# # Inference: Generation of the output\n",
    "# generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "# generated_ids_trimmed = [\n",
    "#     out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "# output_text = processor.batch_decode(\n",
    "#     generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "# )\n",
    "# print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71bff11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chen/dev/.venv/lib/python3.13/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.qwen2_5_vl.configuration_qwen2_5_vl.Qwen2_5_VLConfig'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV3Config, DiffLlamaConfig, ElectraConfig, Emu3Config, ErnieConfig, FalconConfig, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, GitConfig, GlmConfig, Glm4Config, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, JambaConfig, JetMoeConfig, LlamaConfig, Llama4Config, Llama4TextConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, ZambaConfig, Zamba2Config.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# # 加载量化模型\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# model = AutoModelForCausalLM.from_quantized(\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#     model_name,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#     safetensors=True\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mflash_attention_2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda:0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 打印模型信息\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel torch_dtype: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/.venv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:574\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m    571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    572\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    573\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Unrecognized configuration class <class 'transformers.models.qwen2_5_vl.configuration_qwen2_5_vl.Qwen2_5_VLConfig'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV3Config, DiffLlamaConfig, ElectraConfig, Emu3Config, ErnieConfig, FalconConfig, FalconMambaConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, GitConfig, GlmConfig, Glm4Config, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, JambaConfig, JetMoeConfig, LlamaConfig, Llama4Config, Llama4TextConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MistralConfig, MixtralConfig, MllamaConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, XmodConfig, ZambaConfig, Zamba2Config."
     ]
    }
   ],
   "source": [
    "from awq.models.base import BaseAWQForCausalLM\n",
    "# from transformers import AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoProcessor\n",
    "# 使用 AWQ 量化模型路径\n",
    "model_name = \"Qwen/Qwen2.5-VL-3B-Instruct-AWQ\"\n",
    "\n",
    "# 加载 tokenizer 和 processor\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "# # 加载量化模型\n",
    "# model = AutoModelForCausalLM.from_quantized(\n",
    "#     model_name,\n",
    "#     device_map=\"auto\",\n",
    "#     fuse_layers=True,\n",
    "#     trust_remote_code=False,\n",
    "#     safetensors=True\n",
    "# )\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, attn_implementation=\"flash_attention_2\", device_map=\"cuda:0\")\n",
    "\n",
    "# 打印模型信息\n",
    "print(f\"Model torch_dtype: {model.dtype}\")\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\", \n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# 准备推理输入\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# 推理生成输出\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267be31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "# from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# # default: Load the model on the available device(s)\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     # \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "#     \"Qwen/Qwen2.5-VL-3B-Instruct\", \n",
    "#     torch_dtype=\"auto\", \n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "\n",
    "# # print model torch_dtype and device_map\n",
    "# print(f\"Model torch_dtype: {model.dtype}\")\n",
    "# # print(f\"Model device_map: {model.device_map}\")\n",
    "\n",
    "# # We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# # model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "# #     \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "# #     torch_dtype=torch.bfloat16,\n",
    "# #     attn_implementation=\"flash_attention_2\",\n",
    "# #     device_map=\"auto\",\n",
    "# # )\n",
    "\n",
    "# # default processor\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "# # The default range for the number of visual tokens per image in the model is 4-16384.\n",
    "# # You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
    "# # min_pixels = 256*28*28\n",
    "# # max_pixels = 1280*28*28\n",
    "# # processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\n",
    "#                 \"type\": \"image\",\n",
    "#                 \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n",
    "#             },\n",
    "#             {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "#         ],\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# # Preparation for inference\n",
    "# text = processor.apply_chat_template(\n",
    "#     messages, tokenize=False, add_generation_prompt=True\n",
    "# )\n",
    "# image_inputs, video_inputs = process_vision_info(messages)\n",
    "# inputs = processor(\n",
    "#     text=[text],\n",
    "#     images=image_inputs,\n",
    "#     videos=video_inputs,\n",
    "#     padding=True,\n",
    "#     return_tensors=\"pt\",\n",
    "# )\n",
    "# inputs = inputs.to(model.device)\n",
    "\n",
    "# # Inference: Generation of the output\n",
    "# generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "# generated_ids_trimmed = [\n",
    "#     out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "# output_text = processor.batch_decode(\n",
    "#     generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "# )\n",
    "# print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a27cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "# from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# # 加载模型：启用 4-bit 量化（NF4 类型）\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "#     device_map=\"auto\",                      # 自动分配设备（支持多 GPU）\n",
    "#     load_in_4bit=True,                      # 启用 4-bit 量化\n",
    "#     bnb_4bit_quant_type=\"nf4\",              # NF4 是一种适合预训练权重的量化类型\n",
    "#     bnb_4bit_use_double_quant=True,         # 双重量化进一步压缩\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16   # 计算时使用 bfloat16 提升性能\n",
    "# )\n",
    "\n",
    "# # 加载处理器（processor）\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n",
    "\n",
    "# # 示例输入消息\n",
    "# messages = [\n",
    "#     {\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": [\n",
    "#             {\n",
    "#                 \"type\": \"image\",\n",
    "#                 \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\", \n",
    "#             },\n",
    "#             {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "#         ],\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "# # 预处理文本部分\n",
    "# text = processor.apply_chat_template(\n",
    "#     messages, tokenize=False, add_generation_prompt=True\n",
    "# )\n",
    "\n",
    "# # 处理视觉信息（提取图像/视频）\n",
    "# image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "# # 构建最终输入张量\n",
    "# inputs = processor(\n",
    "#     text=[text],\n",
    "#     images=image_inputs,\n",
    "#     videos=video_inputs,\n",
    "#     padding=True,\n",
    "#     return_tensors=\"pt\",\n",
    "# )\n",
    "# inputs = inputs.to(model.device)  # 移动到模型所在设备（如 GPU）\n",
    "\n",
    "# # 生成输出\n",
    "# generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "# # 去除 prompt 部分，仅保留生成内容\n",
    "# generated_ids_trimmed = [\n",
    "#     out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "\n",
    "# # 解码输出\n",
    "# output_text = processor.batch_decode(\n",
    "#     generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "# )\n",
    "\n",
    "# # 打印结果\n",
    "# print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd6a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例输入消息\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\", \n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# 预处理文本部分\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# 处理视觉信息（提取图像/视频）\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "# 构建最终输入张量\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)  # 移动到模型所在设备（如 GPU）\n",
    "\n",
    "# 生成输出\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "# 去除 prompt 部分，仅保留生成内容\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "# 解码输出\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "# 打印结果\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486754d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"\"\"Describe object(s) and (possible) their relative locations in brief with 1 very short sentence:\n",
    "                DO NOT describe background/surface: \n",
    "                for example: \n",
    "                A red phone is under a wood mouse. ; \n",
    "                Only one red cup. ; \n",
    "                The image contains no visible objects\"\"\" \n",
    "\n",
    "# 示例输入消息\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": \"/home/chen/dev/Factory/img/GreenUp1.jpg\", \n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": user_prompt},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# 预处理文本部分\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "# 处理视觉信息（提取图像/视频）\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "\n",
    "# 构建最终输入张量\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)  # 移动到模型所在设备（如 GPU）\n",
    "\n",
    "# 生成输出\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "# 去除 prompt 部分，仅保留生成内容\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "# 解码输出\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "\n",
    "# 打印结果\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09214632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_systemdev313",
   "language": "python",
   "name": "dev_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
