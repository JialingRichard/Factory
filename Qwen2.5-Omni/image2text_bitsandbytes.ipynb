{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1693a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BitsAndBytesConfig' object has no attribute 'get_loading_attributes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mQwen/Qwen2.5-Omni-7B-GPTQ-Int4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 加载模型：启用 4-bit 量化（NF4 类型）\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m model = \u001b[43mQwen2_5OmniForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# 自动分配设备（支持多 GPU）\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_in_4bit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                      \u001b[49m\u001b[38;5;66;43;03m# 启用 4-bit 量化\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_quant_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnf4\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;66;43;03m# NF4 是一种适合预训练权重的量化类型\u001b[39;49;00m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_use_double_quant\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# 双重量化进一步压缩\u001b[39;49;00m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbnb_4bit_compute_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 计算时使用 bfloat16 提升性能\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_implementation\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mflash_attention_2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 使用 Flash Attention 2\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m model.disable_talker()  \u001b[38;5;66;03m# 如果不需要语音输出，禁用 Talker\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 加载处理器\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/.venv/lib/python3.13/site-packages/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py:4378\u001b[39m, in \u001b[36mQwen2_5OmniForConditionalGeneration.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4362\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   4363\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_pretrained\u001b[39m(\n\u001b[32m   4364\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4376\u001b[39m     **kwargs,\n\u001b[32m   4377\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m4378\u001b[39m     model = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4379\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4380\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4381\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4382\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4383\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4384\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4385\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4386\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4388\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4390\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4391\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4392\u001b[39m     spk_path = cached_file(\n\u001b[32m   4393\u001b[39m         pretrained_model_name_or_path,\n\u001b[32m   4394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mspk_dict.pt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4402\u001b[39m         revision=kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mrevision\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m   4403\u001b[39m     )\n\u001b[32m   4404\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m spk_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:309\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    307\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    311\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:4376\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4374\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pre_quantized \u001b[38;5;129;01mor\u001b[39;00m quantization_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4375\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pre_quantized:\n\u001b[32m-> \u001b[39m\u001b[32m4376\u001b[39m         config.quantization_config = \u001b[43mAutoHfQuantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge_quantization_configs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\n\u001b[32m   4378\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4379\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   4380\u001b[39m         config.quantization_config = quantization_config\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/.venv/lib/python3.13/site-packages/transformers/quantizers/auto.py:215\u001b[39m, in \u001b[36mAutoHfQuantizer.merge_quantization_configs\u001b[39m\u001b[34m(cls, quantization_config, quantization_config_from_args)\u001b[39m\n\u001b[32m    206\u001b[39m         quantization_config = AutoQuantizationConfig.from_dict(quantization_config)\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    209\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    210\u001b[39m         quantization_config, (GPTQConfig, AwqConfig, AutoRoundConfig, FbgemmFp8Config, CompressedTensorsConfig)\n\u001b[32m   (...)\u001b[39m\u001b[32m    213\u001b[39m ):\n\u001b[32m    214\u001b[39m     \u001b[38;5;66;03m# special case for GPTQ / AWQ / FbgemmFp8 config collision\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     loading_attr_dict = \u001b[43mquantization_config_from_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loading_attributes\u001b[49m()\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m attr, val \u001b[38;5;129;01min\u001b[39;00m loading_attr_dict.items():\n\u001b[32m    217\u001b[39m         \u001b[38;5;28msetattr\u001b[39m(quantization_config, attr, val)\n",
      "\u001b[31mAttributeError\u001b[39m: 'BitsAndBytesConfig' object has no attribute 'get_loading_attributes'"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "import torch\n",
    "import soundfile as sf\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Omni-7B\"\n",
    "# model_name = \"Qwen/Qwen2.5-Omni-7B-GPTQ-Int4\"\n",
    "# 加载模型：启用 4-bit 量化（NF4 类型）\n",
    "model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",                      # 自动分配设备（支持多 GPU）\n",
    "    load_in_4bit=True,                      # 启用 4-bit 量化\n",
    "    bnb_4bit_quant_type=\"nf4\",              # NF4 是一种适合预训练权重的量化类型\n",
    "    bnb_4bit_use_double_quant=True,         # 双重量化进一步压缩\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # 计算时使用 bfloat16 提升性能\n",
    "    attn_implementation=\"flash_attention_2\" # 使用 Flash Attention 2\n",
    ")\n",
    "model.disable_talker()  # 如果不需要语音输出，禁用 Talker\n",
    "\n",
    "# 加载处理器\n",
    "processor = Qwen2_5OmniProcessor.from_pretrained(model_name)\n",
    "\n",
    "# 图像预处理\n",
    "img_path = \"../img/BlueUp1.jpg\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "image = image.resize((224, 224))  # 或者 (384, 384)\n",
    "image.save(\"resized.jpg\")\n",
    "\n",
    "# 构建对话输入\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant that can understand images and answer questions.\"}],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"resized.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"Describe objects and their relative locations in details: \"}\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "# 多模态信息处理\n",
    "USE_AUDIO_IN_VIDEO = False\n",
    "text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)\n",
    "\n",
    "# 构建输入张量\n",
    "inputs = processor(\n",
    "    text=text,\n",
    "    audio=audios,\n",
    "    images=images,\n",
    "    videos=videos,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    use_audio_in_video=USE_AUDIO_IN_VIDEO,\n",
    ")\n",
    "inputs = inputs.to(model.device).to(model.dtype)  # 移动到模型所在设备\n",
    "\n",
    "# 推理生成文本\n",
    "text_ids = model.generate(\n",
    "    **inputs,\n",
    "    use_audio_in_video=False,\n",
    "    return_audio=False,\n",
    "    max_new_tokens=20,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "# 解码并输出结果\n",
    "output_text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e53bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def inference_img(img_path, \n",
    "                system_prompt=\"You are a helpful assistant that can understand images and answer questions.\", \n",
    "                user_prompt=\"Describe objects and their relative locations in details: \",\n",
    "                resize_location=\"resized.jpg\",\n",
    "                resieze_size=(224, 224)):\n",
    "    img_path = img_path\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image = image.resize(resieze_size) \n",
    "    image.save(resize_location)\n",
    "    \n",
    "    # plot resized image\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # 去掉坐标轴\n",
    "    plt.title(\"Resized Image\")\n",
    "    plt.show()\n",
    "    \n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_prompt}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": resize_location},\n",
    "                {\"type\": \"text\", \"text\": user_prompt}\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    if_return_audio = False  # 如果不涉及音频，设为 False\n",
    "    # 准备推理输入\n",
    "    text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n",
    "    audios, images, videos = process_mm_info(conversation, use_audio_in_video=if_return_audio)\n",
    "    print(f\"audios: {audios}, images: {images}, videos: {videos}\")\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        audio=audios,\n",
    "        images=images,\n",
    "        videos=videos,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        use_audio_in_video=if_return_audio,\n",
    "    )\n",
    "    inputs = inputs.to(model.device).to(model.dtype)\n",
    "\n",
    "    text_ids = model.generate(\n",
    "        **inputs,\n",
    "        use_audio_in_video=if_return_audio,           # 不启用音频\n",
    "        return_audio=False,                 # 不返回音频\n",
    "        # max_new_tokens=50,                # 限制输出长度\n",
    "        # max_length=10, \n",
    "        do_sample=True,                     # 采样生成（非贪心）\n",
    "        temperature=0.5,                    # 控制多样性\n",
    "        top_p=0.9,                          # nucleus sampling\n",
    "        repetition_penalty=1.1              # 减少重复输出\n",
    "    )\n",
    "\n",
    "\n",
    "    output_text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    print(output_text)\n",
    "    \n",
    "inference_img(img_path=\"../img/img1.jpg\", user_prompt=\"Describe objects and their relative locations in details:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10cca32",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_img(img_path=\"../img/img2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b84b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_img(img_path=\"../img/BlueUp1.jpg\")\n",
    "inference_img(img_path=\"../img/GreenUp1.jpg\")\n",
    "inference_img(img_path=\"../img/red cube.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f74a3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_systemdev313",
   "language": "python",
   "name": "dev_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
