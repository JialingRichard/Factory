{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aae5cd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_infer_single.py\n",
    "\"\"\"\n",
    "单张图片推理脚本 - 符合官方README要求的版本\n",
    "用法示例：\n",
    "    python model_infer_single.py --image ./test/xxx.jpg --prompt \"请描述图片中方块的数量、颜色、堆叠和相对位置\"\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor\n",
    "from qwen_omni_utils import process_mm_info\n",
    "import os\n",
    "import gc\n",
    "\n",
    "\n",
    "def load_model(model_name=\"./models--Qwen--Qwen2.5-Omni-3B/snapshots/f75b40e3da2003cdd6e1829b1f420ca70797c34e\"):\n",
    "    \"\"\"加载本地模型和处理器（使用 FP16 减少显存占用）\"\"\"\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True,max_split_size_mb:128\"\n",
    "\n",
    "    print(f\"正在加载模型 {model_name} ...\")\n",
    "\n",
    "    # 清理显存\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        local_files_only=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        use_safetensors=True,\n",
    "    )\n",
    "\n",
    "    # 如果只需要文本输出，可以禁用talker以节省显存\n",
    "    model.disable_talker()\n",
    "\n",
    "    processor = Qwen2_5OmniProcessor.from_pretrained(\n",
    "        model_name,\n",
    "        local_files_only=True\n",
    "    )\n",
    "\n",
    "    print(\"模型加载完成。\")\n",
    "    return model, processor\n",
    "\n",
    "\n",
    "def run_inference(model, processor, image_path, prompt, max_new_tokens=10):\n",
    "    \"\"\"对单张图片和prompt进行推理，返回文本描述\"\"\"\n",
    "\n",
    "    # 推理前清理显存\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # 加载并处理图片\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        original_size = image.size\n",
    "\n",
    "        # 限制图片大小\n",
    "        max_size = 360\n",
    "        if max(image.size) > max_size:\n",
    "            image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
    "            print(f\"图片从 {original_size} 缩放至: {image.size}\")\n",
    "\n",
    "        # 保存处理后的图片（官方示例使用文件路径）\n",
    "        temp_image_path = \"temp_resized_image.jpg\"\n",
    "        image.save(temp_image_path)\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"图片加载失败: {e}\")\n",
    "\n",
    "    # 构建对话格式 - 这是关键！\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\",\n",
    "                 \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\"}\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": temp_image_path},  # 使用文件路径\n",
    "                {\"type\": \"text\", \"text\": prompt}\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    # 设置是否使用视频中的音频（对于图片设置为False）\n",
    "    USE_AUDIO_IN_VIDEO = False\n",
    "\n",
    "    # 应用聊天模板 - 官方要求的步骤\n",
    "    text = processor.apply_chat_template(\n",
    "        conversation,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False\n",
    "    )\n",
    "\n",
    "    # 处理多模态信息 - 官方要求的步骤\n",
    "    audios, images, videos = process_mm_info(\n",
    "        conversation,\n",
    "        use_audio_in_video=USE_AUDIO_IN_VIDEO\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        f\"处理后的信息: audios={len(audios) if audios else 0}, images={len(images) if images else 0}, videos={len(videos) if videos else 0}\")\n",
    "\n",
    "    # 准备输入\n",
    "    inputs = processor(\n",
    "        text=text,\n",
    "        audio=audios,\n",
    "        images=images,\n",
    "        videos=videos,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        use_audio_in_video=USE_AUDIO_IN_VIDEO\n",
    "    )\n",
    "\n",
    "    # 移动到设备\n",
    "    inputs = inputs.to(model.device).to(model.dtype)\n",
    "\n",
    "    print(f\"输入形状: {inputs['input_ids'].shape}\")\n",
    "\n",
    "    # 生成参数\n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"min_new_tokens\": 1,\n",
    "        \"temperature\": 0.7,\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"eos_token_id\": processor.tokenizer.eos_token_id,\n",
    "        \"pad_token_id\": processor.tokenizer.pad_token_id if processor.tokenizer.pad_token_id else processor.tokenizer.eos_token_id,\n",
    "        \"use_cache\": True,\n",
    "        \"use_audio_in_video\": USE_AUDIO_IN_VIDEO,\n",
    "        \"return_audio\": False,  # 只返回文本\n",
    "    }\n",
    "\n",
    "    # 推理\n",
    "    print(\"开始生成...\")\n",
    "    with torch.no_grad():\n",
    "        text_ids = model.generate(**inputs, **generation_kwargs)\n",
    "\n",
    "    # 解码输出\n",
    "    output_text = processor.batch_decode(\n",
    "        text_ids,\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=False\n",
    "    )[0]\n",
    "\n",
    "    # 清理临时文件\n",
    "    if os.path.exists(temp_image_path):\n",
    "        os.remove(temp_image_path)\n",
    "\n",
    "    # 清理内存\n",
    "    del inputs, text_ids\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return output_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87691e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "Qwen2_5OmniToken2WavModel does not support eager attention implementation, fall back to sdpa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型 Qwen/Qwen2.5-Omni-3B ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "565ca1599b3b4d59b9719e2aad7a57b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载完成。\n",
      "图片从 (1280, 1702) 缩放至: (271, 360)\n",
      "处理后的信息: audios=0, images=1, videos=0\n",
      "输入形状: torch.Size([1, 189])\n",
      "开始生成...\n",
      "\n",
      "==== 推理结果 ====\n",
      "system\n",
      "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\n",
      "user\n",
      "Please describe what you see in the image.\n",
      "assistant\n",
      "Well, I see two cubes stacked on top of each other. The one at the bottom is teal or turquoise - looking like it's made out of plastic or some similar material. And then there's another cube on top that's blue.The background seems to be just plain white, which really makes those colors stand out.You know, if you have any more questions about this picture or anything else, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-Omni-3B\"\n",
    "model, processor = load_model(model_name)\n",
    "result = run_inference(model, processor,\"/home/chen/dev/Factory/img/BlueUp4.jpg\", \n",
    "                       \"Please describe what you see in the image.\", \n",
    "                       10)\n",
    "\n",
    "print(\"\\n==== 推理结果 ====\")\n",
    "print(result)\n",
    "\n",
    "# 命令行调用示例：\n",
    "# python model_infer_single.py --image ./image_test/BlueUp4.jpg --prompt \"This is the workspace of a robotic arm. Based on the image, describe the current scene and infer a reasonable sequence of actions to complete the task: Pick up the green cube.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1fbcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片从 (1280, 1702) 缩放至: (271, 360)\n",
      "处理后的信息: audios=0, images=1, videos=0\n",
      "输入形状: torch.Size([1, 189])\n",
      "开始生成...\n",
      "\n",
      "==== 推理结果 ====\n",
      "system\n",
      "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\n",
      "user\n",
      "Please describe what you see in the image.\n",
      "assistant\n",
      "Well, I can't really talk about images directly from your request. But if there's something specific you want to know or discuss about this picture, like colors, shapes, anything else? Let me know!\n"
     ]
    }
   ],
   "source": [
    "result = run_inference(model, processor,\"/home/chen/dev/Factory/img/BlueUp1.jpg\", \n",
    "                       \"Please describe what you see in the image.\", \n",
    "                       10)\n",
    "\n",
    "print(\"\\n==== 推理结果 ====\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "874e26c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片从 (1280, 1702) 缩放至: (271, 360)\n",
      "处理后的信息: audios=0, images=1, videos=0\n",
      "输入形状: torch.Size([1, 189])\n",
      "开始生成...\n",
      "\n",
      "==== 推理结果 ====\n",
      "system\n",
      "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\n",
      "user\n",
      "Please describe what you see in the image.\n",
      "assistant\n",
      "Well, I see two cubes stacked on top of each other against a white background.The one at the bottom is teal or light greenish - blue, and it's slightly smaller than the cube above it which is bright blue.This kind of setup could be for some sort of simple model-making or geometric study.If you have any more questions about this picture or anything else related to it, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "result = run_inference(model, processor,\"/home/chen/dev/Factory/img/BlueUp1.jpg\", \n",
    "                       \"Please describe what you see in the image.\", \n",
    "                       10)\n",
    "\n",
    "print(\"\\n==== 推理结果 ====\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e9e8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_systemdev313",
   "language": "python",
   "name": "dev_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
